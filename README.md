# ğŸ§  GitHub Lab 2 â€“ Automated Model Retraining & Evaluation (CI/CD Pipeline)

## âš™ï¸ Overview

This lab demonstrates a **complete MLOps CI/CD workflow** for retraining, evaluating, testing, and versioning a machine-learning model directly through **GitHub Actions**.

Each time code is pushed to the `main` branch (or triggered manually), the pipeline:

1. Cleans old artifacts
2. Retrains a Random Forest classifier on the Breast Cancer dataset
3. Evaluates the model and logs metrics
4. Runs automated tests
5. Commits new model and metrics back to the repository

---

## ğŸ§© Pipeline Highlights

**Workflow:** `.github/workflows/model_calibration_on_push.yml`

Key stages:
```bash
1ï¸âƒ£ Checkout code
2ï¸âƒ£ Install dependencies
3ï¸âƒ£ Generate timestamp for artifacts
4ï¸âƒ£ Clean old artifacts (models, metrics, data)
5ï¸âƒ£ Retrain model â†’ saves model_<timestamp>_rf_model.joblib
6ï¸âƒ£ Evaluate model â†’ creates metrics/<timestamp>_metrics.json
7ï¸âƒ£ Run unit tests for training & evaluation
8ï¸âƒ£ Commit & push new artifacts to GitHub
```

---

## ğŸ§ª Major Code Enhancements

| Area                   | Original                                             | Modified / Improved                                                              |
| ---------------------- | ---------------------------------------------------- | -------------------------------------------------------------------------------- |
| **Dataset**            | Used `make_classification()` (random synthetic data) | Replaced with real `sklearn.datasets.load_breast_cancer()` dataset               |
| **Model Type**         | DecisionTreeClassifier                               | Upgraded to **RandomForestClassifier**                                           |
| **Timestamp Handling** | Hardcoded / manual                                   | Dynamic timestamp generation per run                                             |
| **File Organization**  | Artifacts scattered                                  | Standardized directories: `models/`, `metrics/`, `data/`                         |
| **Evaluation Script**  | Printed metrics only                                 | Saves clean JSON file: `metrics/<timestamp>_metrics.json`                        |
| **Testing**            | None                                                 | Added full **pytest** coverage: training + evaluation tests                      |
| **GitHub Actions**     | Single push trigger                                  | Added: cleanup, tests, re-scoped variables, manual trigger (`workflow_dispatch`) |
| **Artifact Hygiene**   | Old files persisted between runs                     | Added `Clean old artifacts` step to prevent shape mismatches                     |
| **Automation**         | Manual execution                                     | Full CI/CD retraining pipeline with commit automation                            |

---

## ğŸ§ª Tests Added

**1ï¸âƒ£ `test/test_model_training.py`**

* Verifies trained model exists
* Checks predictions match expected shape

**2ï¸âƒ£ `test/test_evaluate_model.py`**

* Confirms metrics JSON file generation
* Validates metric keys (`accuracy`, `f1_score`)
* Ensures metric values are within 0â€“1 range
* Supports re-evaluation on existing models

---

## ğŸš€ Example CI/CD Run (Successful Log)
```bash
âœ… Retrain Model
Timestamp received from GitHub Actions: 20251112185444
Model saved â†’ models/model_20251112185444_rf_model.joblib
âœ… Training complete.

âœ… Evaluate Model and Log Metrics
Evaluating model for timestamp 20251112185444
âœ… Metrics saved â†’ metrics/20251112185444_metrics.json
âœ… Verified metrics file.

âœ… Run Unit Tests
All 4 tests passed in 1.04s âœ…

âœ… Commit & Push Changes
[main abc123] Add metrics and updated model
2 files changed, 3 insertions(+)
```

---

## ğŸ“ Final Repository Structure
```
Github_Lab2/
â”œâ”€â”€ .github/workflows/
â”‚   â”œâ”€â”€ model_calibration_on_push.yml
â”‚   â””â”€â”€ model_calibration.yml
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ train_model.py
â”‚   â”œâ”€â”€ evaluate_model.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ test/
â”‚   â”œâ”€â”€ test_model_training.py
â”‚   â”œâ”€â”€ test_evaluate_model.py
â”‚   â””â”€â”€ __init__.py
â”œâ”€â”€ models/
â”œâ”€â”€ metrics/
â”œâ”€â”€ data/
â”œâ”€â”€ .env
â”œâ”€â”€ .gitignore
â”œâ”€â”€ README.md
â””â”€â”€ requirements.txt
```

---

## ğŸ§  How to Run Locally
```bash
# 1. Install dependencies
pip install -r requirements.txt

# 2. Train model manually
timestamp=$(date '+%Y%m%d%H%M%S')
python src/train_model.py --timestamp "$timestamp"

# 3. Evaluate model
python src/evaluate_model.py --timestamp "$timestamp"

# 4. Run tests
pytest -v test/
```

---

## âš¡ Triggering the Workflow

* **Automatic:** on every push to `main`
* **Manual:** via the "Run workflow" button under *Actions â†’ Model Retraining on Push to Main*

---

## ğŸ Outcome

After each successful run:

* New model and metrics are saved in their respective folders
* JSON metrics are versioned
* Tests validate performance and structure
* Artifacts are committed automatically to the repo

âœ… **Fully reproducible, tested, and version-controlled ML retraining workflow.**

---

**Author:** *Priyanka Senthil*  
**Updated:** November 2025  
**Environment:** Python 3.9 | scikit-learn | MLflow | GitHub Actions | Pytest

---